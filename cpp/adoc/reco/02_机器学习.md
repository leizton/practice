--------------------------------------------------------------------------------------------------------------
# 最小二乘法 least squares
最小化误差平方和, 误差函数(目标函数) E = ∑ (f - y)²
设拟合函数(回归函数, 模型)是 f = k·x + b
通过对E求k、b偏导 找到极值点
∂E/∂k = ∑ 2(k·x + b - y)·x = 0
∂E/∂b = ∑ 2(k·x + b - y)   = 0
两个方程解两个未知数

# 加权最小二乘法 weighted least square
高斯马尔科夫定理 Gauss–Markov theorem
  在统计学的线性回归模型中，如果残差满足零均值、相同方差、相互独立，则回归系数的BLUE是普通最小二乘估计
  BLUE: 最佳线性无偏估计 best linear unbiased estimator
  比如服务的qps监控指标，每个时间点的残差(实际值减估计值)是一个随机变量，随时间变化的随机变量就是随机过程
  每个时间点的残差都有自己的均值和方差，当没有异常点时，就满足上述三个条件的前两个
实际应用场景中的问题
  有时会有突增流量，突增点的残差的均值、方差和正常点就不同，突则方差更大、增则均值大于0
  异常点对估计有影响
解决思路
  加权最小二乘就是考虑了方差变化情况下的线性估计
  目标函数 E = ∑ (1/σ²)·(f - y)², 权重是方差的倒数, 起到降突的作用
具体解法
  可导时对目标函数求偏导找极值点
tips
  误差指整体预测结果和实际的差值，残差是样本级的概念

# 迭代重加权最小二乘法 IRLS Iteratively reweighted least square
使用场景
  目标函数具有p范数(p-norm)形式的优化问题, argmin(β){ ∑ |f(β) - y|ᵖ }
  比如, 线性回归模型的最大似然估计(MLE)
作用和意义
  加权LS具有消除数据集中异常点对估计的影响的意义，但权重没有具体的函数形式，所以难以直接求解析解
  同时对于p=1的|e|这种不可导的情况，因此引入迭代方法
具体解法
  把所有样本的残差方程 eᵢ = f(xᵢ) - yᵢ 变成矩阵形式 e = Ab - y
  e是残差向量, y是样本观测值向量, b是模型参数向量
  例如 f = b₁·x + b₂, A·b = [x₁ 1; x₂ 1]·[b₁; b₂]
  因此优化问题(optimal), 或者说最大似然估计, 转成了求方程 Ax-y=0 的近似解 xᶺ
  (tips: 参数b变成了未知数x)
和加权LS的关系
  加权前是最小化|e|², 加权后是最小化|W·e|²=|W·Ax - W·y|², W是标准差构成的对角矩阵
  本质上还是 Ax-y=0, 只是 A=W·A, y=W·y
和p范数优化的关系
  目标函数 ∑|eᵢ|ᵖ = ∑(|eᵢ|ᵖ⁻² · eᵢ²) = ∑(wᵢ² · eᵢ²) = |W·e|², 其中 wᵢ²=|eᵢ₋₁|ᵖ⁻²
  若p=1, wᵢ² = 1 / |eᵢ₋₁|, 上一次误差越大则本次权重越小
@ref
  监控系统中的自动检测 AIOps, https://www.infoq.cn/article/mjqlkcrs1sbgbria2dhx

# 为什么不用线性回归解决分类问题
分类问题
  考虑用肿瘤大小分类恶性和良性，特征只有一个即肿瘤大小
线性回归如何解分类问题
  线性回归拟合一条直线y=kx+b, x是肿瘤大小, 当样本是恶性时 实际值yᶺ=1
  拟合结果是得到一条直线, 使所有点到直线的距离和最小
  对于新样本x, 设这条直线给出的值是y, 这个y值并反映属于某个分类的概率
  所以我们还需要把y转成概率, 例如用如下分段函数
    P(yᶺ=1┆y<0)=0, P(yᶺ=1┆y>1)=1, P(yᶺ=1┆1>y>0)=y
  这个分段函数就是逻辑回归的雏形
逻辑回归
  上面的分段函数有一个明显的缺陷: 当两个分类的方差不同时, 分类效果变差
  设良性数据位于中心x₁半径r₁的圆内, 恶性数据位于C(x₂,r₂)
    最佳分类点是 (x₁+r₁) (x₂-r₂) 的中点, 而上面的分段函数给出的分类点是 x₁ x₂ 的中点
  可以看出实际需要学习的是分段函数的中心点和斜率
  可学习即连续可导, 所以逻辑回归函数设计成 LG(x)=1/(1+exp(-kx+b)), k控制斜率 b控制中心点
  这个函数可以看成线性函数与激活函数的复合
@ref
  <<可解释机器学习>>