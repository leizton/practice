--------------------------------------------------------------------------------------------------------------
# 最小二乘法 least squares
最小化误差平方和, 误差函数(目标函数) E = ∑ (f - y)²
设拟合函数(回归函数, 模型)是 f = k·x + b
通过对E求k、b偏导 找到极值点
∂E/∂k = ∑ 2(k·x + b - y)·x = 0
∂E/∂b = ∑ 2(k·x + b - y)   = 0
两个方程解两个未知数

# 加权最小二乘法 weighted least square
高斯马尔科夫定理 Gauss–Markov theorem
  在统计学的线性回归模型中，如果残差满足零均值、相同方差、相互独立，则回归系数的BLUE是普通最小二乘估计
  BLUE: 最佳线性无偏估计 best linear unbiased estimator
  比如服务的qps监控指标，每个时间点的残差(实际值减估计值)是一个随机变量，随时间变化的随机变量就是随机过程
  每个时间点的残差都有自己的均值和方差，当没有异常点时，就满足上述三个条件的前两个
实际应用场景中的问题
  有时会有突增流量，突增点的残差的均值、方差和正常点就不同，突则方差更大、增则均值大于0
  异常点对估计有影响
解决思路
  加权最小二乘就是考虑了方差变化情况下的线性估计
  目标函数 E = ∑ (1/σ²)·(f - y)², 权重是方差的倒数, 起到降突的作用
具体解法
  可导时对目标函数求偏导找极值点
tips
  误差指整体预测结果和实际的差值，残差是样本级的概念

# 迭代重加权最小二乘法 IRLS Iteratively reweighted least square
使用场景
  目标函数具有p范数(p-norm)形式的优化问题, argmin(β){ ∑ |f(β) - y|ᵖ }
  比如, 线性回归模型的最大似然估计(MLE)
作用和意义
  加权LS具有消除数据集中异常点对估计的影响的意义，但权重没有具体的函数形式，所以难以直接求解析解
  同时对于p=1的|e|这种不可导的情况，因此引入迭代方法
具体解法
  把所有样本的残差方程 eᵢ = f(xᵢ) - yᵢ 变成矩阵形式 e = Ab - y
  e是残差向量, y是样本观测值向量, b是模型参数向量
  例如 f = b₁·x + b₂, A·b = [x₁ 1; x₂ 1]·[b₁; b₂]
  因此优化问题(optimal), 或者说最大似然估计, 转成了求方程 Ax-y=0 的近似解 xᶺ
  (tips: 参数b变成了未知数x)
和加权LS的关系
  加权前是最小化|e|², 加权后是最小化|W·e|²=|W·Ax - W·y|², W是标准差构成的对角矩阵
  本质上还是 Ax-y=0, 只是 A=W·A, y=W·y
和p范数优化的关系
  目标函数 ∑|eᵢ|ᵖ = ∑(|eᵢ|ᵖ⁻² · eᵢ²) = ∑(wᵢ² · eᵢ²) = |W·e|², 其中 wᵢ²=|eᵢ₋₁|ᵖ⁻²
  若p=1, wᵢ² = 1 / |eᵢ₋₁|, 上一次误差越大则本次权重越小
@ref
  监控系统中的自动检测 AIOps, https://www.infoq.cn/article/mjqlkcrs1sbgbria2dhx

# 为什么不用线性回归解决分类问题
分类问题
  考虑用肿瘤大小分类恶性和良性，特征只有一个即肿瘤大小
线性回归如何解分类问题
  线性回归拟合一条直线y=kx+b, x是肿瘤大小, 当样本是恶性时 实际值yᶺ=1
  拟合结果是得到一条直线, 使所有点到直线的距离和最小
  对于新样本x, 设这条直线给出的值是y, 这个y值并反映属于某个分类的概率
  所以我们还需要把y转成概率, 例如用如下分段函数
    P(yᶺ=1┆y<0)=0, P(yᶺ=1┆y>1)=1, P(yᶺ=1┆1>y>0)=y
  这个分段函数就是逻辑回归的雏形
逻辑回归
  上面的分段函数有一个明显的缺陷: 当两个分类的方差不同时, 分类效果变差
  设良性数据位于中心x₁半径r₁的圆内, 恶性数据位于C(x₂,r₂)
    最佳分类点是 (x₁+r₁) (x₂-r₂) 的中点, 而上面的分段函数给出的分类点是 x₁ x₂ 的中点
  可以看出实际需要学习的是分段函数的中心点和斜率
  学习即迭代收敛的过程，现有学习方法(如梯度下降法)的必要条件是连续可导
  所以逻辑回归函数设计成 LG(x)=1/(1+exp(-kx+b)), 与分段函数形似
  b控制中心点 k控制斜率, 分类问题的学习结果就是最佳分类点和确信度
  这个函数可以看成线性函数与激活函数的复合
@ref
  <<可解释机器学习>>

# 岭回归 Lasso回归
解决过拟合问题
损失函数加上参数的 l2 l1 norm
  岭回归损失函数 ∑(f - y)² + ∑θ²
  Lasso回归损失函数 ∑(f - y)² + ∑|θ|
@ref
  https://zhuanlan.zhihu.com/p/88698511

# 广义线性模型 GLM generalized linear model
线性模型的使用场景(局限性、缺点)
  1. 真实值符合高斯分布
    预测值是真实值的期望(均值)，残差符合正态分布
    例如用面积预测某城市房价，面积是一个特征，房价是预测对象，整体趋势上房价和面积呈正线性相关
    但不同位置单价不同，当单价呈高斯分布时预测效果好
  2. 特征间相互独立
    例如预测房价时面积和楼层高度是两个相互独立的特征，但面积和单价有一定关系
    其他条件相同是，面积越大单价越小，因此需要考虑联合概率
  3. 特征与结果之间是线性关系
    例如房价和面积不完全是线性关系，面积过大时会降低单价
对应的解决方法
  1. 保留特征加权和，用非线性函数连接加权和与分布的期望
    例如逻辑回归中用逻辑函数连接，实际上线性模型拟合一条直线，逻辑函数把加权和映射到伯努利分布的期望
    加权和越大，分布的期望越接近1
  2. 手动添加一个交互特征
    新加的交互特征是原始两个特征的乘积
  3. GAM 学习样条函数的权重
    GAM: generalized additive model 广义加性模型
    理论上样条函数的无限组合可以拟合任意函数，例如函数可分解成傅里叶级数
