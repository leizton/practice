# 两种模型
CBOW      通过 周围词 预测 中心词
skip-gram 通过 中心词 预测 周围词

# 网络结构
in-out
  输入词的one-hot, 预测输出词的one-hot
  输入词即context, 可以是1个也可以是多个
softmax
  从二分类逻辑回归扩展到多分类
  最大池化是选出最大的一个类赋予概率1, 其余类赋予概率0
  softmax保留每个类概率的相对大小信息, 同时提升大的概率值, 打压小的概率值

# 选择隐层左边矩阵还是右边矩阵当作embed_vec
以CBOW举例
  相似的中心词具有相似的context, 中心词在out层, 所以hidden的输出相似
  因此out层与hidden层的连接也应相似, 所以CBOW的embed_vec选择右边矩阵
规律
  中心词在输出层, 则选择右边矩阵
  中心词在输入层, 则选择左边矩阵